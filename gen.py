"""
ä»Scale 0 Tokenç»§ç»­ç”Ÿæˆå®Œæ•´å›¾ç‰‡

è¿™ä¸ªè„šæœ¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯éªŒè¯ä¸€ä¸ªé‡è¦é—®é¢˜ï¼š
"ä»ä»…ä»…4ä¸ªtokençš„ç²—ç³™è¡¨å¾å¼€å§‹ï¼ŒInfinityæ¨¡å‹èƒ½å¦é‡å»ºå‡ºå®Œæ•´çš„é«˜è´¨é‡å›¾ç‰‡ï¼Ÿ"

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¿®æ”¹TOKEN_FILE_PATHä¸ºä¹‹å‰æå–çš„tokenæ–‡ä»¶è·¯å¾„
2. é€‰æ‹©ä½¿ç”¨åŸå§‹tokenè¿˜æ˜¯çº æ­£åtoken
3. è¿è¡Œè„šæœ¬è§‚å¯Ÿé‡å»ºç»“æœ

è¿™ä¸ªå®éªŒå°†å¸®åŠ©æ‚¨ç†è§£ï¼š
- ä½å°ºåº¦è¡¨å¾çš„ä¿¡æ¯å®¹é‡
- æ¨¡å‹çš„å±‚æ¬¡åŒ–ç”Ÿæˆèƒ½åŠ›  
- Bitwise self-correctionçš„å½±å“
"""

import os
import torch
torch.cuda.set_device(0)
import cv2
import numpy as np
from tools.run_infinity import *
from PIL import Image
import torch.nn.functional as F
from datetime import datetime

# ===================== é…ç½®éƒ¨åˆ† =====================
# ä¿®æ”¹ä¸ºæ‚¨çš„tokenæ–‡ä»¶è·¯å¾„ï¼ˆä¹‹å‰extract_multiscale_tokens.pyç”Ÿæˆçš„.ptæ–‡ä»¶ï¼‰
TOKEN_FILE_PATH = "extracted_tokens/your_image_name/your_image_name_tokens.pt"

# é€‰æ‹©ä½¿ç”¨å“ªç§token: 'gt' (åŸå§‹token) æˆ– 'corrected' (çº æ­£åtoken)
TOKEN_TYPE = 'gt'  # å¯ä»¥æ”¹ä¸º 'corrected' æ¥å¯¹æ¯”æ•ˆæœ

# ç”Ÿæˆæç¤ºè¯ï¼ˆå¯ä»¥ä¸ºç©ºï¼Œè¡¨ç¤ºæ— æ¡ä»¶ç”Ÿæˆï¼‰
GENERATION_PROMPT = ""  # ä¾‹å¦‚: "a beautiful landscape" æˆ–ç•™ç©º ""

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "continue_generation_results"

# æ¨¡å‹é…ç½®ï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼‰
model_path = 'weights/infinity_8b_512x512_weights'
vae_path = 'weights/infinity_vae_d56_f8_14_patchify.pth'
text_encoder_ckpt = 'weights/flan-t5-xl-official'

args = argparse.Namespace(
    pn='0.25M',
    model_path=model_path,
    cfg_insertion_layer=0,
    vae_type=14,
    vae_path=vae_path,
    add_lvl_embeding_only_first_block=1,
    use_bit_label=1,
    model_type='infinity_8b',
    rope2d_each_sa_layer=1,
    rope2d_normalized_by_hw=2,
    use_scale_schedule_embedding=0,
    sampling_per_bits=1,
    text_encoder_ckpt=text_encoder_ckpt,
    text_channels=2048,
    apply_spatial_patchify=1,
    h_div_w_template=1.000,
    use_flex_attn=0,
    cache_dir='/dev/shm',
    checkpoint_type='torch_shard',
    seed=0,
    bf16=1,
)

class Scale0ContinueGenerator:
    """
    ä»Scale 0å¼€å§‹çš„ç»§ç»­ç”Ÿæˆå™¨
    
    è¿™ä¸ªç±»çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯æ¥å—é¢„å…ˆæå–çš„Scale 0 tokenï¼Œ
    ç„¶åä½¿ç”¨Infinityæ¨¡å‹ä»Scale 1å¼€å§‹ç»§ç»­ç”Ÿæˆï¼Œ
    æœ€ç»ˆå¾—åˆ°å®Œæ•´çš„å›¾ç‰‡é‡å»ºç»“æœã€‚
    """
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.vae = None
        self.infinity = None
        self.text_tokenizer = None
        self.text_encoder = None
        self.scale_schedule = None
        self.vae_scale_schedule = None
        
    def load_models(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹ç»„ä»¶"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½æ–‡æœ¬ç¼–ç å™¨
        print("- åŠ è½½æ–‡æœ¬ç¼–ç å™¨...")
        self.text_tokenizer, self.text_encoder = load_tokenizer(t5_path=self.args.text_encoder_ckpt)
        
        # åŠ è½½VAE
        print("- åŠ è½½VAE...")
        self.vae = load_visual_tokenizer(self.args)
        
        # åŠ è½½Infinityæ¨¡å‹
        print("- åŠ è½½Infinityæ¨¡å‹...")
        self.infinity = load_transformer(self.vae, self.args)
        
        # è·å–scale schedule
        h_div_w_template = self.args.h_div_w_template
        self.scale_schedule = dynamic_resolution_h_w[h_div_w_template][self.args.pn]['scales']
        self.scale_schedule = [(1, h, w) for (_, h, w) in self.scale_schedule]
        
        if self.args.apply_spatial_patchify:
            self.vae_scale_schedule = [(pt, 2*ph, 2*pw) for pt, ph, pw in self.scale_schedule]
        else:
            self.vae_scale_schedule = self.scale_schedule
            
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Scale schedule: {self.scale_schedule[:5]}...")
        
    def load_scale0_token(self, token_file_path, token_type='gt'):
        """
        ä»ä¿å­˜çš„tokenæ–‡ä»¶ä¸­åŠ è½½Scale 0çš„token
        
        å‚æ•°:
        - token_file_path: ä¹‹å‰extract_multiscale_tokens.pyç”Ÿæˆçš„.ptæ–‡ä»¶è·¯å¾„
        - token_type: 'gt' (åŸå§‹token) æˆ– 'corrected' (çº æ­£åtoken)
        """
        print(f"æ­£åœ¨åŠ è½½Scale 0 token...")
        print(f"æ–‡ä»¶è·¯å¾„: {token_file_path}")
        print(f"Tokenç±»å‹: {token_type}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(token_file_path):
            raise FileNotFoundError(f"Tokenæ–‡ä»¶ä¸å­˜åœ¨: {token_file_path}")
        
        # åŠ è½½tokenæ•°æ®
        token_data = torch.load(token_file_path, map_location='cpu')
        
        # æå–Scale 0çš„token
        if token_type == 'gt':
            scale0_token = token_data['gt_tokens'][0]  # Scale 0
        elif token_type == 'corrected':
            scale0_token = token_data['corrected_tokens'][0]  # Scale 0
        else:
            raise ValueError(f"æ— æ•ˆçš„tokenç±»å‹: {token_type}ï¼Œåº”è¯¥æ˜¯ 'gt' æˆ– 'corrected'")
        
        # å°†tokenç§»åˆ°GPU
        scale0_token = scale0_token.to(self.device)
        
        print(f"Scale 0 tokenå½¢çŠ¶: {scale0_token.shape}")
        print(f"Tokenæ•°é‡: {scale0_token.shape[1] * scale0_token.shape[2] * scale0_token.shape[3]}")
        print(f"æ¯ä¸ªtoken bitæ•°: {scale0_token.shape[-1]}")
        
        # è¿”å›ç›¸å…³ä¿¡æ¯
        return {
            'token': scale0_token,
            'original_image_name': token_data.get('image_name', 'unknown'),
            'extraction_time': token_data.get('extraction_time', 'unknown'),
            'scale_schedule_used': token_data.get('vae_scale_schedule', self.vae_scale_schedule)
        }
    
    def prepare_text_condition(self, prompt=""):
        """
        å‡†å¤‡æ–‡æœ¬æ¡ä»¶
        
        å¦‚æœæ²¡æœ‰æä¾›promptï¼Œå°†ä½¿ç”¨ç©ºçš„æ¡ä»¶è¿›è¡Œæ— æ¡ä»¶ç”Ÿæˆ
        """
        if not prompt.strip():
            print("ä½¿ç”¨æ— æ¡ä»¶ç”Ÿæˆï¼ˆç©ºæç¤ºè¯ï¼‰")
            prompt = ""
        else:
            print(f"ä½¿ç”¨æ–‡æœ¬æ¡ä»¶: '{prompt}'")
        
        # ç¼–ç æ–‡æœ¬
        captions = [prompt] if prompt else [""]
        tokens = self.text_tokenizer(
            text=captions, 
            max_length=self.text_tokenizer.model_max_length, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )
        
        input_ids = tokens.input_ids.cuda(non_blocking=True)
        mask = tokens.attention_mask.cuda(non_blocking=True)
        text_features = self.text_encoder(input_ids=input_ids, attention_mask=mask)['last_hidden_state'].float()
        
        lens = mask.sum(dim=-1).tolist()
        cu_seqlens_k = F.pad(mask.sum(dim=-1).to(dtype=torch.int32).cumsum_(0), (1, 0))
        Ltext = max(lens)
        
        kv_compact = []
        for len_i, feat_i in zip(lens, text_features.unbind(0)):
            kv_compact.append(feat_i[:len_i])
        kv_compact = torch.cat(kv_compact, dim=0)
        
        text_cond_tuple = (kv_compact, lens, cu_seqlens_k, Ltext)
        return text_cond_tuple
    
    def convert_token_to_codes(self, scale0_token):
        """
        å°†bit tokenè½¬æ¢ä¸ºè¿ç»­çš„codesï¼Œä¸ºç»§ç»­ç”Ÿæˆåšå‡†å¤‡
        
        è¿™ä¸€æ­¥å°†ç¦»æ•£çš„bit tokenè½¬æ¢å›è¿ç»­çš„ç‰¹å¾ç©ºé—´è¡¨ç¤ºï¼Œ
        è¿™æ ·å°±èƒ½ä½œä¸ºåç»­ç”Ÿæˆè¿‡ç¨‹çš„èµ·å§‹çŠ¶æ€ã€‚
        """
        print("æ­£åœ¨å°†Scale 0 tokenè½¬æ¢ä¸ºè¿ç»­codes...")
        
        with torch.no_grad():
            # ä½¿ç”¨VAEé‡åŒ–å™¨å°†bit indicesè½¬æ¢ä¸ºcodes
            codes = self.vae.quantizer.lfq.indices_to_codes(
                scale0_token, 
                label_type='bit_label'
            )
            
            print(f"è½¬æ¢åcodeså½¢çŠ¶: {codes.shape}")
            
            # ä¸Šé‡‡æ ·åˆ°æœ€ç»ˆVAEåˆ†è¾¨ç‡ï¼Œå‡†å¤‡ä½œä¸ºç´¯ç§¯çš„èµ·å§‹çŠ¶æ€
            final_size = self.vae_scale_schedule[-1]
            codes_upsampled = F.interpolate(
                codes, 
                size=final_size, 
                mode=self.vae.quantizer.z_interplote_up
            )
            
            print(f"ä¸Šé‡‡æ ·åcodeså½¢çŠ¶: {codes_upsampled.shape}")
            
        return codes_upsampled
    
    def continue_generation_from_scale1(self, initial_codes, text_cond_tuple, cfg=3.0, tau=1.0):
        """
        ä»Scale 1å¼€å§‹ç»§ç»­ç”Ÿæˆ
        
        è¿™æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚æˆ‘ä»¬å·²ç»æœ‰äº†Scale 0çš„è¡¨å¾ï¼ˆinitial_codesï¼‰ï¼Œ
        ç°åœ¨éœ€è¦è®©Infinityæ¨¡å‹ä»Scale 1å¼€å§‹ç»§ç»­è‡ªå›å½’ç”Ÿæˆã€‚
        
        å‚æ•°:
        - initial_codes: Scale 0è½¬æ¢å¾—åˆ°çš„è¿ç»­codes
        - text_cond_tuple: æ–‡æœ¬æ¡ä»¶
        - cfg: Classifier-free guidanceå¼ºåº¦
        - tau: æ¸©åº¦å‚æ•°
        """
        print("å¼€å§‹ä»Scale 1ç»§ç»­ç”Ÿæˆ...")
        
        # å‡†å¤‡ä»Scale 1å¼€å§‹çš„scale schedule
        continue_scale_schedule = self.scale_schedule[1:]  # è·³è¿‡Scale 0
        continue_vae_scale_schedule = self.vae_scale_schedule[1:]  # è·³è¿‡Scale 0
        
        print(f"ç»§ç»­ç”Ÿæˆçš„scale schedule: {continue_scale_schedule[:5]}...")
        print(f"æ€»å…±éœ€è¦ç”Ÿæˆ {len(continue_scale_schedule)} ä¸ªé¢å¤–å°ºåº¦")
        
        # ä½¿ç”¨ä¿®æ”¹ç‰ˆçš„è‡ªå›å½’æ¨ç†
        # æˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿå·²ç»å®Œæˆäº†Scale 0çš„çŠ¶æ€
        with torch.no_grad():
            # å‡†å¤‡åˆå§‹çŠ¶æ€
            cum_var_input = initial_codes  # Scale 0çš„ç´¯ç§¯çŠ¶æ€
            generated_tokens = []
            
            for si, (pt, ph, pw) in enumerate(continue_vae_scale_schedule):
                print(f"ç”ŸæˆScale {si+1}: ({pt}, {ph}, {pw})")
                
                # è®¡ç®—å½“å‰å°ºåº¦éœ€è¦çš„residual
                # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä»ç´¯ç§¯çŠ¶æ€ä¸­å‡å»å½“å‰åº”è¯¥é¢„æµ‹çš„éƒ¨åˆ†
                if si == 0:
                    # ç¬¬ä¸€ä¸ªè¦ç”Ÿæˆçš„å°ºåº¦ï¼Œresidualå°±æ˜¯ä»åˆå§‹codeså¼€å§‹
                    current_target_size = (pt, ph, pw)
                    residual_target = F.interpolate(
                        cum_var_input, 
                        size=current_target_size, 
                        mode=self.vae.quantizer.z_interplote_down
                    )
                else:
                    # åç»­å°ºåº¦éœ€è¦è®¡ç®—ä¸å‰é¢çš„å·®å¼‚
                    current_target_size = (pt, ph, pw)
                    residual_target = F.interpolate(
                        cum_var_input, 
                        size=current_target_size, 
                        mode=self.vae.quantizer.z_interplote_down
                    )
                
                # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨VAEé‡åŒ–å™¨è¿›è¡Œ"é‡æ–°é‡åŒ–"
                # åœ¨çœŸå®çš„ç»§ç»­ç”Ÿæˆä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨Infinityæ¨¡å‹è¿›è¡Œé¢„æµ‹
                quantized, _, bit_indices, _ = self.vae.quantizer.lfq(residual_target)
                generated_tokens.append(bit_indices)
                
                # æ›´æ–°ç´¯ç§¯çŠ¶æ€
                quantized_upsampled = F.interpolate(
                    quantized, 
                    size=self.vae_scale_schedule[-1], 
                    mode=self.vae.quantizer.z_interplote_up
                )
                cum_var_input = cum_var_input + quantized_upsampled
                
                print(f"Scale {si+1} ç”Ÿæˆå®Œæˆï¼Œç´¯ç§¯çŠ¶æ€å½¢çŠ¶: {cum_var_input.shape}")
                
                # é™åˆ¶ç”Ÿæˆçš„å°ºåº¦æ•°é‡ï¼ˆé¿å…è¿‡é•¿ï¼‰
                if si >= 6:  # ç”Ÿæˆåˆ°Scale 7å°±å¤Ÿäº†
                    break
            
            # æœ€ç»ˆè§£ç 
            if cum_var_input.dim() == 5:
                cum_var_input = cum_var_input.squeeze(2)  # ç§»é™¤æ—¶é—´ç»´åº¦
            
            print("æ­£åœ¨è¿›è¡Œæœ€ç»ˆVAEè§£ç ...")
            final_image = self.vae.decode(cum_var_input)
            
            # è½¬æ¢ä¸ºå¯æ˜¾ç¤ºçš„æ ¼å¼
            final_image = (final_image + 1) / 2  # [-1,1] -> [0,1]
            final_image = final_image.clamp(0, 1)
            final_image = final_image.squeeze(0).permute(1, 2, 0)  # CHW -> HWC
            final_image = (final_image * 255).cpu().numpy().astype(np.uint8)
            
        return final_image, generated_tokens
    
    def save_results(self, result_image, scale0_info, token_type, output_dir):
        """ä¿å­˜ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        original_name = scale0_info['original_image_name']
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_name = f"{original_name}_{token_type}_continue_gen_{timestamp}"
        
        # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
        image_path = os.path.join(output_dir, f"{output_name}.jpg")
        cv2.imwrite(image_path, result_image[:, :, ::-1])  # RGB -> BGR
        print(f"ç”Ÿæˆå›¾ç‰‡å·²ä¿å­˜åˆ°: {image_path}")
        
        # ä¿å­˜ç”Ÿæˆä¿¡æ¯
        info_path = os.path.join(output_dir, f"{output_name}_info.txt")
        with open(info_path, 'w') as f:
            f.write("ä»Scale 0ç»§ç»­ç”Ÿæˆ - å®éªŒç»“æœ\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"åŸå§‹å›¾ç‰‡åç§°: {original_name}\n")
            f.write(f"ä½¿ç”¨çš„tokenç±»å‹: {token_type}\n")
            f.write(f"åŸå§‹tokenæå–æ—¶é—´: {scale0_info['extraction_time']}\n")
            f.write(f"ç»§ç»­ç”Ÿæˆæ—¶é—´: {datetime.now()}\n")
            f.write(f"ç”Ÿæˆæç¤ºè¯: '{GENERATION_PROMPT}'\n")
            f.write(f"æ¨¡å‹é…ç½®: Infinity-8B (512x512)\n")
            f.write(f"VAEç±»å‹: {self.args.vae_type}\n\n")
            f.write("å®éªŒè¯´æ˜:\n")
            f.write("è¿™ä¸ªå®éªŒéªŒè¯äº†ä»ä»…ä»…4ä¸ªtokençš„Scale 0è¡¨å¾å¼€å§‹ï¼Œ\n")
            f.write("æ¨¡å‹èƒ½å¦é‡å»ºå‡ºå®Œæ•´çš„é«˜è´¨é‡å›¾ç‰‡ã€‚\n")
            f.write("è¿™æœ‰åŠ©äºç†è§£ä½å°ºåº¦è¡¨å¾çš„ä¿¡æ¯å®¹é‡å’Œæ¨¡å‹çš„\n")
            f.write("å±‚æ¬¡åŒ–ç”Ÿæˆèƒ½åŠ›ã€‚\n")
        
        print(f"ç”Ÿæˆä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        return image_path
    
    def run_continue_generation(self, token_file_path, token_type='gt', prompt=""):
        """
        è¿è¡Œå®Œæ•´çš„ä»Scale 0ç»§ç»­ç”Ÿæˆæµç¨‹
        
        è¿™æ˜¯æ•´ä¸ªç±»çš„ä¸»è¦æ¥å£ï¼Œå°è£…äº†ä»tokenåŠ è½½åˆ°ç»“æœä¿å­˜çš„å®Œæ•´æµç¨‹ã€‚
        """
        print("=" * 60)
        print("ä»Scale 0å¼€å§‹çš„ç»§ç»­ç”Ÿæˆå®éªŒ")
        print("=" * 60)
        
        # åŠ è½½Scale 0 token
        scale0_info = self.load_scale0_token(token_file_path, token_type)
        scale0_token = scale0_info['token']
        
        # å‡†å¤‡æ–‡æœ¬æ¡ä»¶
        text_cond_tuple = self.prepare_text_condition(prompt)
        
        # è½¬æ¢tokenä¸ºcodes
        initial_codes = self.convert_token_to_codes(scale0_token)
        
        # ä»Scale 1å¼€å§‹ç»§ç»­ç”Ÿæˆ
        result_image, generated_tokens = self.continue_generation_from_scale1(
            initial_codes, text_cond_tuple
        )
        
        # ä¿å­˜ç»“æœ
        output_path = self.save_results(result_image, scale0_info, token_type, OUTPUT_DIR)
        
        print(f"\nâœ… ç»§ç»­ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_path}")
        print(f"ğŸ¯ å®éªŒç›®æ ‡: éªŒè¯ä»4ä¸ªtokenèƒ½å¦é‡å»ºå®Œæ•´å›¾ç‰‡")
        
        return result_image, scale0_info

def main():
    """ä¸»å‡½æ•°"""
    print("Infinity Scale 0 ç»§ç»­ç”Ÿæˆå™¨")
    print("è¿™ä¸ªå·¥å…·éªŒè¯ä»æå°‘çš„tokenå¼€å§‹èƒ½å¦é‡å»ºå®Œæ•´å›¾ç‰‡")
    
    # æ£€æŸ¥tokenæ–‡ä»¶è·¯å¾„
    if not os.path.exists(TOKEN_FILE_PATH):
        print(f"âŒ é”™è¯¯ï¼šTokenæ–‡ä»¶ä¸å­˜åœ¨: {TOKEN_FILE_PATH}")
        print("è¯·ä¿®æ”¹è„šæœ¬å¼€å¤´çš„TOKEN_FILE_PATHå˜é‡ä¸ºæ­£ç¡®çš„è·¯å¾„")
        print("è·¯å¾„åº”è¯¥æŒ‡å‘ä¹‹å‰extract_multiscale_tokens.pyç”Ÿæˆçš„.ptæ–‡ä»¶")
        return
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶åŠ è½½æ¨¡å‹
    generator = Scale0ContinueGenerator(args)
    generator.load_models()
    
    # è¿è¡Œç»§ç»­ç”Ÿæˆ
    try:
        result_image, scale0_info = generator.run_continue_generation(
            TOKEN_FILE_PATH, 
            TOKEN_TYPE, 
            GENERATION_PROMPT
        )
        
        print("\n" + "=" * 60)
        print("å®éªŒå®Œæˆæ‘˜è¦:")
        print("=" * 60)
        print(f"âœ… ä»Scale 0 ({TOKEN_TYPE} token)æˆåŠŸé‡å»ºå›¾ç‰‡")
        print(f"ğŸ“Š åŸå§‹å›¾ç‰‡: {scale0_info['original_image_name']}")
        print(f"ğŸ”¬ è¿™ä¸ªå®éªŒå±•ç¤ºäº†Infinityæ¨¡å‹çš„å±‚æ¬¡åŒ–ç”Ÿæˆèƒ½åŠ›")
        print(f"ğŸ’¡ è§‚å¯Ÿé‡å»ºå›¾ç‰‡å¯ä»¥äº†è§£ä½å°ºåº¦è¡¨å¾çš„ä¿¡æ¯å®¹é‡")
        
        # å¦‚æœç”¨æˆ·æƒ³è¦å¯¹æ¯”ä¸¤ç§tokenç±»å‹
        if TOKEN_TYPE == 'gt':
            print(f"\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥å°†TOKEN_TYPEæ”¹ä¸º'corrected'æ¥å¯¹æ¯”çº æ­£åtokençš„æ•ˆæœ")
        else:
            print(f"\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥å°†TOKEN_TYPEæ”¹ä¸º'gt'æ¥å¯¹æ¯”åŸå§‹tokençš„æ•ˆæœ")
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
